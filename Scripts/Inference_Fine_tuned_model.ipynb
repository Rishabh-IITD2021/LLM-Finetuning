{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "782ca378",
      "metadata": {
        "id": "782ca378"
      },
      "source": [
        "## Use Saved LLAMA Model\n",
        "\n",
        "- Model finetuned on 400 samples generated from relevance documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d51477",
      "metadata": {
        "id": "07d51477"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec03df2b",
      "metadata": {
        "id": "ec03df2b"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import pipeline, BitsAndBytesConfig,  LlamaForCausalLM, LlamaTokenizer, Conversation,logging\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1bbe42d",
      "metadata": {
        "id": "f1bbe42d"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a63e80",
      "metadata": {
        "id": "40a63e80",
        "outputId": "21c8dd5e-0b6b-4247-d383-11f9c28cb253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 4.77 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Model and tokenizer names\n",
        "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "refined_model = \"./llama-2-7b-hf-e2r-finetuned/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48981a8",
      "metadata": {
        "id": "b48981a8"
      },
      "source": [
        "## Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f0022a",
      "metadata": {
        "id": "c8f0022a",
        "outputId": "17af4ff0-b527-4a42-8efa-aa4ce0be036c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 43.8 ms, sys: 7.96 ms, total: 51.8 ms\n",
            "Wall time: 123 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Tokenizer\n",
        "llama_tokenizer =  LlamaTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\"  # Fix for fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ebe1dc",
      "metadata": {
        "id": "e9ebe1dc"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08bf185",
      "metadata": {
        "id": "e08bf185",
        "outputId": "9b04f655-aaf5-44d4-82ac-37c954f2b4dc",
        "colab": {
          "referenced_widgets": [
            "656cc1ba54af4a618eb4b4715233a4b9"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "656cc1ba54af4a618eb4b4715233a4b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.76 s, sys: 3.71 s, total: 8.47 s\n",
            "Wall time: 3min 17s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Quantization Config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "# Model\n",
        "base_model =  LlamaForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfef383b",
      "metadata": {
        "id": "cfef383b"
      },
      "source": [
        "## Load LORA adapter and merge with model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1ad627",
      "metadata": {
        "id": "2f1ad627",
        "outputId": "e8dd8dbf-acc3-49b8-9b99-f0872b2afa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 362 µs, sys: 210 µs, total: 572 µs\n",
            "Wall time: 381 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "config = PeftConfig.from_pretrained(refined_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4385171",
      "metadata": {
        "id": "c4385171",
        "outputId": "bb5736c0-f40e-4b51-fbf9-175f84468865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5.09 s, sys: 353 ms, total: 5.44 s\n",
            "Wall time: 5.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = PeftModel.from_pretrained(base_model, refined_model, offload_folder='./offload_dir', device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750c7a1c",
      "metadata": {
        "id": "750c7a1c"
      },
      "source": [
        "# Get Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7451ffd6",
      "metadata": {
        "id": "7451ffd6"
      },
      "outputs": [],
      "source": [
        "def getResponse(output):\n",
        "    text_gen = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=llama_tokenizer,\n",
        "                        max_length=200,\n",
        "                        device_map=\"auto\")\n",
        "\n",
        "    output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
        "    output = output[0]['generated_text']\n",
        "\n",
        "    first_inst = output.find('[/INST]')\n",
        "    second_inst = output.find('[/INST]', output.find('[/INST]') + 1)\n",
        "\n",
        "    print(first_inst, second_inst)\n",
        "    if first_inst != -1 and second_inst != -1:\n",
        "        return output[first_inst + 7: second_inst]\n",
        "    else:\n",
        "        print(\"No answer found\")\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682dcfd2",
      "metadata": {
        "id": "682dcfd2",
        "outputId": "71ebbc9d-e6aa-440e-d799-1b84757bf7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 843 µs, sys: 9 µs, total: 852 µs\n",
            "Wall time: 857 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Optional\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import warnings\n",
        "from transformers import logging\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextGenerationInference:\n",
        "    model: torch.nn.Module\n",
        "    tokenizer: Any\n",
        "    use_int4: Optional[bool] = False\n",
        "    use_int8: Optional[bool] = False\n",
        "    temperature: Optional[float] = 1.0\n",
        "    top_k: Optional[int] = 50\n",
        "    top_p: Optional[float] = 0.95\n",
        "    repetition_penalty: Optional[float] = 1.0\n",
        "    num_return_sequences: Optional[int] = 1\n",
        "    num_beams: Optional[int] = 1\n",
        "    max_new_tokens: Optional[int] = 1024\n",
        "    do_sample: Optional[bool] = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.model.eval()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def chat(self, prompt):\n",
        "        inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            temperature=self.temperature,\n",
        "            top_k=self.top_k,\n",
        "            top_p=self.top_p,\n",
        "            repetition_penalty=self.repetition_penalty,\n",
        "            num_return_sequences=self.num_return_sequences,\n",
        "            num_beams=self.num_beams,\n",
        "            #max_length=self.max_new_tokens,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            do_sample=self.do_sample,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "        )\n",
        "\n",
        "        output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8464223a",
      "metadata": {
        "id": "8464223a"
      },
      "outputs": [],
      "source": [
        "query = \"Write the exact BigFixRelevance for the following description: \\\n",
        "Check the application anaconda is installed?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf4c03f",
      "metadata": {
        "id": "bbf4c03f",
        "outputId": "92579715-bb13-4ff7-c587-bf8b71c89668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance>not exists regapp \"anaconda.exe\"</Relevance> \n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "inference_engine = TextGenerationInference(model=model, tokenizer=llama_tokenizer)\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9fce131",
      "metadata": {
        "id": "a9fce131"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70258f8",
      "metadata": {
        "id": "b70258f8"
      },
      "outputs": [],
      "source": [
        "llama_client = TextGenerationInference(model=model, tokenizer=llama_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf3934a",
      "metadata": {
        "id": "5cf3934a"
      },
      "source": [
        "# Question Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9606b5dd",
      "metadata": {
        "id": "9606b5dd",
        "outputId": "fbdf1276-dd60-4df8-d64e-6215ab562d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No ans found\n",
            "118 195\n",
            "CPU times: user 1min, sys: 116 ms, total: 1min\n",
            "Wall time: 1min\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' <</Relevance><Relevance>not exists regapp \"anaconda.exe\"</Relevance> '"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description: \\\n",
        "Check the application anaconda is installed?\"\n",
        "getResponse(llama_client.chat(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4d979e",
      "metadata": {
        "id": "9e4d979e"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7fddcf5",
      "metadata": {
        "id": "b7fddcf5",
        "outputId": "2587807b-202f-4ed6-cd88-b377034045d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</RELEVANCE> <INST>logged in user : logged in user</INST> \n",
            "CPU times: user 17.7 s, sys: 84 ms, total: 17.8 s\n",
            "Wall time: 17.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Locate the currently logged in users.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13429427",
      "metadata": {
        "id": "13429427"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6683c40",
      "metadata": {
        "id": "e6683c40",
        "outputId": "4dd151d5-29ea-47e9-c7cc-b8fec3b09ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</RElevance><RElevance>logged on user : logged on user</RElevance> \n",
            "CPU times: user 13.3 s, sys: 87.9 ms, total: 13.4 s\n",
            "Wall time: 13.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Discover who is currently logged into the system.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b6cf56",
      "metadata": {
        "id": "13b6cf56"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "071d0aa2",
      "metadata": {
        "id": "071d0aa2",
        "outputId": "d6b9b467-4cd7-4785-c3d4-797e5223f6ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance> signed in user <string> of <active directory server> </Relevance> [/Relevance] \n",
            "CPU times: user 50.4 s, sys: 67.9 ms, total: 50.5 s\n",
            "Wall time: 50.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Identify the users currently signed in.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf56bdc",
      "metadata": {
        "id": "1bf56bdc"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8587f28",
      "metadata": {
        "id": "c8587f28",
        "outputId": "d05c5f24-3015-4252-97ab-c0d5e0ce3ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance>logged on user <string> of <temp> of runtime</Relevance> \n",
            "CPU times: user 51.4 s, sys: 63.8 ms, total: 51.4 s\n",
            "Wall time: 51.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Determine the users that are presently logged on.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364e2cef",
      "metadata": {
        "id": "364e2cef"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4655fb",
      "metadata": {
        "id": "7f4655fb",
        "outputId": "0bab20dd-9251-4047-9f4a-c1b033161a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance>active of <system> : boolean</Relevance> \n",
            "CPU times: user 5.55 s, sys: 20 ms, total: 5.57 s\n",
            "Wall time: 5.56 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Find out who is actively using the system.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec5dae3",
      "metadata": {
        "id": "9ec5dae3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a938533",
      "metadata": {
        "id": "0a938533",
        "outputId": "1a995fc4-c21c-4257-c628-6491e1296368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance> logged in user : logged in user</Relevance> \n",
            "CPU times: user 51.4 s, sys: 44 ms, total: 51.5 s\n",
            "Wall time: 51.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Pinpoint the users who are logged in.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389074dc",
      "metadata": {
        "id": "389074dc"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9fed997",
      "metadata": {
        "id": "b9fed997",
        "outputId": "9e6125b3-62c2-480c-acbe-fd8c4f219f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance> list of current users</Relevance> \n",
            "CPU times: user 19.6 s, sys: 35.9 ms, total: 19.7 s\n",
            "Wall time: 19.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Uncover the list of users currently signed into the platform.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5834d13c",
      "metadata": {
        "id": "5834d13c"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29eae1aa",
      "metadata": {
        "id": "29eae1aa",
        "outputId": "7a094d2d-d975-443c-ff4e-f423a5ddf69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <</Relevance> <Relevance> logged on user : logged on user</Relevance> \n",
            "CPU times: user 12.7 s, sys: 52 ms, total: 12.7 s\n",
            "Wall time: 12.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Retrieve information on the users currently logged on.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51de6d09",
      "metadata": {
        "id": "51de6d09"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a64f4ae",
      "metadata": {
        "id": "4a64f4ae",
        "outputId": "65feddc2-68fd-47b4-df94-bf8b94810fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <br/> <INST>logged on individual : logged on individual</INST> \n",
            "CPU times: user 19.9 s, sys: 36 ms, total: 19.9 s\n",
            "Wall time: 19.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Search for the logged on individuals.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bed7008",
      "metadata": {
        "id": "1bed7008"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da02c7c",
      "metadata": {
        "id": "2da02c7c",
        "outputId": "95773fe7-44c0-452e-d96d-6092999a4573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " <div> <inst><relevance>list of <logged in user> : logged in user</relevance></div> \n",
            "CPU times: user 8.82 s, sys: 76 ms, total: 8.9 s\n",
            "Wall time: 8.89 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Generate Text\n",
        "query = \"Write the exact BigFixRelevance for the following description:Establish a list of the currently logged in users.\"\n",
        "# getResponse(llama_client.chat(query))\n",
        "response = inference_engine.chat(f\"<s>[INST] {query} [/INST]\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb28145",
      "metadata": {
        "id": "8cb28145"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (whatever you want to call it)",
      "language": "python",
      "name": "envname"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}